# ============================================================================
# CI/CD - Training Job via Vast.ai (On-Demand)
# ============================================================================
name: Training Job

on:
  workflow_dispatch:
    inputs:
      epochs:
        description: 'N√∫mero de epochs'
        default: '50'
        type: string
      batch_size:
        description: 'Batch size'
        default: '16'
        type: string
      patience:
        description: 'Early stopping patience'
        default: '10'
        type: string
      all_time:
        description: 'Usar TODAS las inferencias (ignorar √∫ltimo training)'
        default: false
        type: boolean

permissions:
  contents: read
  id-token: write

env:
  IMAGE_NAME: "training"
  WORKLOAD_IDENTITY_PROVIDER: "projects/${{ secrets.GCP_PROJECT_NUMBER }}/locations/global/workloadIdentityPools/github-actions-pool/providers/github-provider"

jobs:
  # ================================================================
  # Job 1: Exportar datos verificados a GCS
  # ================================================================
  export:
    name: Export Verified Data to GCS
    runs-on: ubuntu-latest
    environment: deploy

    steps:
      - name: Authenticate to GCP
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: ${{ env.WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ vars.SERVICE_ACCOUNT }}

      - name: Get GKE credentials
        uses: google-github-actions/get-gke-credentials@v2
        with:
          cluster_name: ${{ vars.GKE_CLUSTER }}
          location: ${{ vars.GCP_ZONE }}

      - name: Get API endpoint
        run: |
          API_IP=$(kubectl get svc inference-api-service -n waste-detection -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
          echo "API_URL=http://$API_IP:8000" >> $GITHUB_ENV
          echo "üì° API URL: http://$API_IP:8000"

      - name: Export verified data to GCS
        run: |
          RESPONSE=$(curl -s -X POST "$API_URL/training/export-to-gcs?min_detections=1")
          echo "$RESPONSE" | python3 -m json.tool
          
          STATUS=$(echo "$RESPONSE" | python3 -c "import sys,json; print(json.load(sys.stdin).get('status','error'))")
          TOTAL=$(echo "$RESPONSE" | python3 -c "import sys,json; print(json.load(sys.stdin).get('total_records',0))")
          
          if [ "$STATUS" = "empty" ]; then
            echo "‚ùå No hay datos verificados para entrenar"
            exit 1
          fi
          
          echo "‚úÖ Exportados $TOTAL registros verificados a GCS"

  # ================================================================
  # Job 2: Training en Vast.ai
  # ================================================================
  train:
    name: Run Training (Vast.ai)
    runs-on: ubuntu-latest
    needs: export
    environment: deploy

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Authenticate to GCP
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: ${{ env.WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ vars.SERVICE_ACCOUNT }}

      - name: Generate GCS access token
        run: |
          ACCESS_TOKEN=$(gcloud auth print-access-token)
          echo "GCS_ACCESS_TOKEN=$ACCESS_TOKEN" >> $GITHUB_ENV
          echo "‚úÖ Access token generado"

      - name: Install Vast.ai CLI
        run: |
          pip install vastai
          vastai set api-key ${{ secrets.VAST_API_KEY }}

      - name: Build training args
        run: |
          ARGS="--from-inferences --only-verified --epochs=${{ inputs.epochs }} --batch-size=${{ inputs.batch_size }} --patience=${{ inputs.patience }}"
          if [ "${{ inputs.all_time }}" = "true" ]; then
            ARGS="$ARGS --all-time"
          fi
          echo "TRAINING_ARGS=$ARGS" >> $GITHUB_ENV

      - name: Find and launch Vast.ai instance
        run: |
          OFFER_ID=$(vastai search offers 'gpu_ram>=12 num_gpus=1 inet_down>=200 reliability>0.95 verified=true' \
            -o 'dph+' --raw | python3 -c "
          import sys, json
          offers = json.load(sys.stdin)
          if offers:
              print(offers[0]['id'])
          else:
              print('NONE')
          ")
          
          if [ "$OFFER_ID" = "NONE" ]; then
            echo "‚ùå No se encontraron ofertas disponibles en Vast.ai"
            exit 1
          fi
          
          echo "üñ•Ô∏è Oferta seleccionada: $OFFER_ID"
          
          RESULT=$(vastai create instance $OFFER_ID \
            --image ${{ vars.ARTIFACT_REGISTRY }}/${{ env.IMAGE_NAME }}:latest \
            --disk 20 \
            --env "-e GCS_MODELS_BUCKET=${{ vars.GCS_MODELS_BUCKET }} -e GCS_IMAGES_BUCKET=${{ vars.GCS_IMAGES_BUCKET }} -e GCS_DATASETS_BUCKET=${{ vars.GCS_DATASETS_BUCKET }} -e GCS_ACCESS_TOKEN=$GCS_ACCESS_TOKEN -e TRAINING_ARGS='$TRAINING_ARGS'" \
            --onstart-cmd "cd /app && python train.py \$TRAINING_ARGS && touch /workspace/TRAINING_COMPLETE" \
            --raw)
          
          INSTANCE_ID=$(echo "$RESULT" | python3 -c "import sys,json; print(json.load(sys.stdin).get('new_contract',''))")
          echo "INSTANCE_ID=$INSTANCE_ID" >> $GITHUB_ENV
          echo "‚úÖ Instancia lanzada: $INSTANCE_ID"

      - name: Wait for training to complete
        run: |
          echo "‚è≥ Esperando que el training termine (timeout: 2h)..."
          TIMEOUT=7200
          ELAPSED=0
          
          while [ $ELAPSED -lt $TIMEOUT ]; do
            STATUS=$(vastai show instance $INSTANCE_ID --raw | python3 -c "
          import sys, json
          data = json.load(sys.stdin)
          print(data.get('actual_status', 'unknown'))
          " 2>/dev/null || echo "unknown")
            
            echo "  Estado: $STATUS (${ELAPSED}s)"
            
            if [ "$STATUS" = "exited" ] || [ "$STATUS" = "offline" ]; then
              echo "‚úÖ Instancia termin√≥"
              break
            fi
            
            sleep 60
            ELAPSED=$((ELAPSED + 60))
          done
          
          if [ $ELAPSED -ge $TIMEOUT ]; then
            echo "‚ùå Timeout: el training excedi√≥ las 2 horas"
          fi

      - name: Show training logs
        if: always()
        run: |
          vastai logs $INSTANCE_ID --tail 100 2>/dev/null || echo "No se pudieron obtener los logs"

      - name: Destroy Vast.ai instance
        if: always()
        run: |
          if [ -n "$INSTANCE_ID" ]; then
            vastai destroy instance $INSTANCE_ID
            echo "üóëÔ∏è Instancia $INSTANCE_ID destruida"
          fi

  # ================================================================
  # Job 3: Reiniciar API
  # ================================================================
  deploy:
    name: Deploy New Model
    runs-on: ubuntu-latest
    needs: train
    environment: deploy

    steps:
      - name: Authenticate to GCP
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: ${{ env.WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ vars.SERVICE_ACCOUNT }}

      - name: Get GKE credentials
        uses: google-github-actions/get-gke-credentials@v2
        with:
          cluster_name: ${{ vars.GKE_CLUSTER }}
          location: ${{ vars.GCP_ZONE }}

      - name: Restart inference API
        run: |
          kubectl rollout restart deployment/inference-api -n waste-detection
          kubectl rollout status deployment/inference-api -n waste-detection --timeout=300s
          echo "‚úÖ API reiniciada con el nuevo modelo"