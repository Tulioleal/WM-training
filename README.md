# Waste Detection Training Service

This repository contains the training service for the YOLOv8-based Waste Detection model. It is designed to perform both initial and incremental training using data from Google Cloud Storage (GCS) and tracking progress via a PostgreSQL database.

## Features

- **YOLOv8 Training**: Utilizes the Ultralytics framework for high-performance object detection.
- **Incremental Training**: Capable of training on new data (inferences) generated by the production model.
- **GCS Integration**: Automatically downloads datasets and base models from GCS and uploads trained models and metadata.
- **Database Logging**: Records training sessions, metrics, and model versions in a PostgreSQL database.
- **Cloud-Native**: Optimized Docker image designed for Kubernetes execution with GPU support.

## Getting Started

### Prerequisites

- Python 3.10
- Docker
- NVIDIA GPU (for training)
- Google Cloud Platform account (for GCS access)

### Installation

1. Clone the repository:
   ```bash
   git clone <repository-url>
   cd WM-training
   ```

2. (Optional) Create a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   # Install torch with CUDA support
   pip install torch==2.1.0+cu121 torchvision==0.16.0+cu121 --index-url https://download.pytorch.org/whl/cu121
   ```

## Configuration

The service is configured via environment variables:

| Variable | Description |
|----------|-------------|
| `GCS_MODELS_BUCKET` | Bucket name for storing models |
| `GCS_DATASETS_BUCKET` | Bucket name for source datasets |
| `GCS_IMAGES_BUCKET` | Bucket name containing inference images and labels |
| `DATABASE_URL` | PostgreSQL connection string |
| `GCP_PROJECT_ID` | Google Cloud Project ID |
| `GCS_ACCESS_TOKEN` | (Optional) Access token for GCS |

## Usage

### Training from a pre-structured dataset

```bash
python train.py --dataset gs://your-bucket/path/to/dataset --epochs 50
```

### Training from inferences (Incremental)

```bash
python train.py --from-inferences --only-verified --epochs 30
```

**Arguments:**
- `--epochs`: Number of training epochs (default: 50).
- `--batch-size`: Batch size (default: 16).
- `--from-inferences`: Enable training from saved inferences.
- `--only-verified`: Only use inferences that have been verified or corrected.
- `--all-time`: Use all available inferences instead of only new ones since last training.

## Deployment

The service is intended to run as a Kubernetes Job. A sample configuration is provided in `k8s/training-job.yaml`.

To trigger a training job:
```bash
kubectl apply -f k8s/training-job.yaml
```

## Project Structure

- `train.py`: Main entry point for the training service.
- `Dockerfile`: Optimized multi-stage build for GPU environments.
- `requirements.txt`: Python dependencies.
- `k8s/`: Kubernetes manifest files.
